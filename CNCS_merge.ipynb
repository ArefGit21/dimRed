{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNCS-merge",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArefGit21/dimRed/blob/master/CNCS_merge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwM21ygkj1Tq"
      },
      "source": [
        " **Utilizing Gradient Boosting prediction models for predicting age and sex in : \n",
        "HCP participants based on resting-state fMRI networks**\n",
        "\n",
        "Yogi & Friends | 2020, 2021 | CNCS \n",
        "\n",
        " *Aref Mahjoubfar, Morteza Mirjebreili, Farhan Abbasi,\n",
        "Shadi Azizi, Reyhane Vali, Arezoo Saeedi, Sara Tarvand, Milad Reyhani*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drbGQ2rSivZd"
      },
      "source": [
        "!pip install boto3\n",
        "!pip install nipype\n",
        "!pip install nilearn\n",
        "!pip install catboost\n",
        "!pip install nibabel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAZIkRI46GgX"
      },
      "source": [
        "**Import Required libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "on2S7kACsD9p"
      },
      "source": [
        "import boto3\n",
        "import re\n",
        "import os\n",
        "from nilearn import datasets\n",
        "import requests\n",
        "from nilearn.decomposition import CanICA\n",
        "import numpy as np\n",
        "import nibabel as nb\n",
        "import pandas as pd\n",
        "from catboost import CatBoostClassifier, Pool\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwkIfHkY5yQK"
      },
      "source": [
        "**Download Data from HCP Server via s3 Amazon**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOnhJmz5Uhxa"
      },
      "source": [
        "\n",
        "access_key = 'AKIAXO65CT57EQTONEGA'\n",
        "secret_key = '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'\n",
        "\n",
        "\n",
        "def ensure_dir(file_path):\n",
        "    directory = os.path.dirname(file_path)\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  s3_client = boto3.client('s3', aws_access_key_id=access_key, aws_secret_access_key=secret_key)\n",
        "  s3_resource = boto3.resource('s3', aws_access_key_id=access_key, aws_secret_access_key=secret_key)\n",
        "  dataset = 'HCP_1200/'\n",
        "  pattern = 'HCP_1200\\/[0-9]+\\/MNINonLinear\\/Results\\/rfMRI_REST1_LR\\/rfMRI_REST1_LR.nii.gz'\n",
        "  bucket = s3_resource.Bucket('hcp-openaccess')\n",
        "  s3_keys = bucket.objects.filter(Prefix=dataset)\n",
        "  s3_keylist = []\n",
        "  n_subjects = 5\n",
        "  counter = 0\n",
        "  print('Listing starts ...')\n",
        "  for key in s3_keys:\n",
        "    if re.fullmatch(pattern, key.key):\n",
        "      counter +=1\n",
        "      s3_keylist.append(key.key)\n",
        "      if counter>= n_subjects:\n",
        "        break\n",
        "  print('Listing finished')\n",
        "  print('Downloading starts ...')\n",
        "  s3 = boto3.client('s3', aws_access_key_id=access_key, aws_secret_access_key=secret_key)\n",
        "  for key in s3_keylist:\n",
        "    print(key)\n",
        "    path = key.replace('/', os.sep)\n",
        "    ensure_dir(path)\n",
        "    with open(path, 'wb') as data:\n",
        "        s3.download_fileobj('hcp-openaccess', key, data)\n",
        "  print('Downloading finished')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7c8mqtsxiev"
      },
      "source": [
        "**Load Data & Listing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2Qv3Zgmkc5c"
      },
      "source": [
        "#LOAD files & Listing\n",
        "imgcounter=0\n",
        "subject_list=[]\n",
        "subject_dataobj_list=[]\n",
        "subject_header_list=[]\n",
        "subject_ids = []\n",
        "for subdir, dirs, files in os.walk(r'HCP_1200'):\n",
        "    for filename in files:\n",
        "        subject_ids.append(subdir[9:15])\n",
        "        filepath = subdir + os.sep + filename\n",
        "        imgcounter+=1\n",
        "        print(\"------------imgcounter=\",imgcounter,\"------------\") #counter debuger\n",
        "        print(\"Path:\",filepath) #file path debuger\n",
        "        imgfile = os.path.join(filepath)\n",
        "        img = nb.load(imgfile)\n",
        "        subject_list.append(img)\n",
        "        subject_dataobj_list.append(img.dataobj)\n",
        "        print(\"subject_list:\",subject_list) #subject list debuger\n",
        "        subject_header_list.append(img.header)\n",
        "        print(\"subject_header_list:\",subject_header_list) #header dubuger\n",
        "        print(\"subject_dataobj_list:\",subject_dataobj_list)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnKG7YLP5Kvc"
      },
      "source": [
        "\n",
        "**ICA : Deriving Independent Components**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDwZwE7EjCgn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "6367a537-0841-4c99-d1cd-ddae7437f4bd"
      },
      "source": [
        "#check with sample data from nilearn dataset\n",
        "\"\"\"rest_dataset = datasets.fetch_development_fmri(n_subjects=10)\n",
        "func_filenames = rest_dataset.func  # list of 4D nifti files for each subject\n",
        "print(func_filenames)\n",
        "# print basic information on the dataset\n",
        "print('First functional nifti image (4D) is at: %s' %\n",
        "      rest_dataset.func[0])  # 4D data\n",
        "    img = ICASAF(func_filenames)  \"\"\"\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"rest_dataset = datasets.fetch_development_fmri(n_subjects=10)\\nfunc_filenames = rest_dataset.func  # list of 4D nifti files for each subject\\nprint(func_filenames)\\n# print basic information on the dataset\\nprint('First functional nifti image (4D) is at: %s' %\\n      rest_dataset.func[0])  # 4D data\\n    img = ICASAF(func_filenames)  \""
            ]
          },
          "metadata": {},
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3S5OOAn9jNGw"
      },
      "source": [
        "def ICASAF (x):\n",
        "  from nilearn.decomposition import CanICA\n",
        "  canica = CanICA(n_components=20,\n",
        "                memory=\"nilearn_cache\", memory_level=2,\n",
        "                verbose=10,\n",
        "                mask_strategy='template',\n",
        "                random_state=0)\n",
        "  return np.asanyarray(canica.fit_transform(x))\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7sMXSLwtejK"
      },
      "source": [
        "subject_list = ICASAF(subject_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asxwE_pmjt6_"
      },
      "source": [
        "subject_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQZB69EV5QcC"
      },
      "source": [
        "\n",
        "**MODEL AND CROSS VALIDATION**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrvB_e-bqbrK"
      },
      "source": [
        "url = 'https://drive.google.com/uc?id=1BbZugD99dpsM_OotT9OljOagIby3K7bX&export=download'\n",
        "label = pd.read_csv(url,error_bad_lines=False)\n",
        "label = label[['Subject','Gender']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjum7TCzrd80"
      },
      "source": [
        "subject_ids = list(map(int, subject_ids))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m35T1C6AnKKH"
      },
      "source": [
        "label=label[label['Subject'].apply(lambda x: x in subject_ids)]\n",
        "label['Gender'] = label['Gender'].apply(lambda x: 1 if x == 'M' else 0)\n",
        "label = label['Gender']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-hQh3FJuNXu"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(img.reshape(10,-1), label, test_size=0.30, random_state=42)\n",
        "\n",
        "model = CatBoostClassifier(iterations=2,\n",
        "                           depth=2,\n",
        "                           learning_rate=1,\n",
        "                           loss_function='Logloss',\n",
        "                           verbose=True)\n",
        "model.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0tcgN8_ttHc"
      },
      "source": [
        "y_pred = model.predict(X_test)\n",
        "acc = accuracy_score(y_test,y_pred)\n",
        "print(f\"Accuracy of model: {acc}\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}